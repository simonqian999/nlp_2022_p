{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afecec3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:42:40.506139Z",
     "start_time": "2022-04-27T13:42:40.436108Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "# os.environ['BEARER_TOKEN'] = '<WRITE YOUR TOKEN HERE!!!!!>'\n",
    "bearer_token = os.environ.get('BEARER_TOKEN')\n",
    "\n",
    "def create_url(_ids):\n",
    "    ids = \"ids={}\".format(_ids)\n",
    "    tweet_fields = \"tweet.fields=id,text,author_id,conversation_id,created_at,referenced_tweets,geo,in_reply_to_user_id,lang,possibly_sensitive,public_metrics\"\n",
    "    expansion = \"expansions=author_id\"\n",
    "    user_fields = \"user.fields=id,name,username,created_at,description,entities,location,url,verified,withheld\"\n",
    "    \n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}&{}&{}\".format(ids, tweet_fields, expansion, user_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_tweets(_ids):\n",
    "    url = create_url(_ids)\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'Authorization': 'Bearer {}'.format(bearer_token),\n",
    "      'Cookie': 'guest_id=v1%3A164999832374703747; guest_id_ads=v1%3A164999832374703747; guest_id_marketing=v1%3A164999832374703747; personalization_id=\"v1_+F68isE/iukb7yr8y66bOw==\"'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b7489d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T14:44:10.418657Z",
     "start_time": "2022-05-02T14:44:08.471659Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(file_name):\n",
    "    '''\n",
    "    This function will read the file containing tweet ids sperating by lines (in groups)\n",
    "    '''\n",
    "    all_ids = pd.read_table(file_name,sep='\\n',header=None)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db10d2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:34.443837Z",
     "start_time": "2022-04-27T13:11:34.432844Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def separate_ids(ids):\n",
    "    '''\n",
    "    This function will split the sperate ids into a list\n",
    "    '''\n",
    "    seperated_ids = ids.split(\",\")\n",
    "    return seperated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85180d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T13:39:11.506584Z",
     "start_time": "2022-04-28T13:39:11.481276Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_tweet_groups(groups_ids):\n",
    "    '''\n",
    "    This function will get tweets information using Twitter API in groups with indexing\n",
    "    params groups_ids: all ids in lines (groups) retrieved from the orginal file \n",
    "    '''\n",
    "    tweet_groups = {}\n",
    "    for group_index in range(len(groups_ids)):\n",
    "        group_separated_ids = separate_ids(groups_ids[0][group_index])\n",
    "        group_dict = {}\n",
    "        \n",
    "        # first check if the first tweet is valid to be accessed\n",
    "        # if no access, only 'errors' will be returned\n",
    "        first_tweet = get_tweets(group_separated_ids[0])\n",
    "        if len(first_tweet) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            # check if the group contains more than 100 ids \n",
    "            # since multiple tweets lookup has 100 maxmimum\n",
    "            if len(group_separated_ids) > 100:\n",
    "                index = 0\n",
    "                partial_group = {}\n",
    "\n",
    "                # partition groups with more than 100 ids into maximum 100 each group\n",
    "                for i in range(len(group_separated_ids)//100 + 1):\n",
    "                    partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "                    partial_group = get_tweets(partial_group_ids)\n",
    "                    # initialise the dict with the information from the first partial group\n",
    "                    if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "                        group_dict = partial_group\n",
    "                    # combining partial groups into a whole group\n",
    "                    else:\n",
    "                        # there is any tweet valid to be accessed in the partial group\n",
    "                        if len(partial_group) != 1:\n",
    "                            group_dict['data'].extend(partial_group['data'])\n",
    "                            group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "                    index += 100\n",
    "\n",
    "            else:\n",
    "                full_group_ids = ','.join(group_separated_ids)\n",
    "                group_dict = get_tweets(full_group_ids)\n",
    "\n",
    "            tweet_groups[group_index] = group_dict\n",
    "    return tweet_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0f152b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:36.598838Z",
     "start_time": "2022-04-27T13:11:36.577838Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_tweet_groups_start_end(groups_ids, start, end):\n",
    "    '''\n",
    "    This function will get tweets information using Twitter API in groups with indexing\n",
    "    params groups_ids: all ids in lines (groups) retrieved from the orginal file \n",
    "    '''\n",
    "    tweet_groups = {}\n",
    "    for group_index in range(start, end):\n",
    "        group_separated_ids = separate_ids(groups_ids[0][group_index])\n",
    "        group_dict = {}\n",
    "        \n",
    "        # first check if the first tweet is valid to be accessed\n",
    "        # if no access, only 'errors' will be returned\n",
    "        first_tweet = get_tweets(group_separated_ids[0])\n",
    "        if len(first_tweet) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            # check if the group contains more than 100 ids \n",
    "            # since multiple tweets lookup has 100 maxmimum\n",
    "            if len(group_separated_ids) > 100:\n",
    "                index = 0\n",
    "                partial_group = {}\n",
    "\n",
    "                # partition groups with more than 100 ids into maximum 100 each group\n",
    "                for i in range(len(group_separated_ids)//100 + 1):\n",
    "                    if len(group_separated_ids) % 100 == 0 and i == len(group_separated_ids)//100:\n",
    "                        continue\n",
    "                        \n",
    "                    partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "                    partial_group = get_tweets(partial_group_ids)\n",
    "\n",
    "                    # initialise the dict with the information from the first partial group\n",
    "                    if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "                        group_dict = partial_group\n",
    "                    # combining partial groups into a whole group\n",
    "                    else:\n",
    "                        # there is any tweet valid to be accessed in the partial group\n",
    "                        if len(partial_group) != 1:\n",
    "                            group_dict['data'].extend(partial_group['data'])\n",
    "                            group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "                    index += 100\n",
    "\n",
    "            else:\n",
    "                full_group_ids = ','.join(group_separated_ids)\n",
    "                group_dict = get_tweets(full_group_ids)\n",
    "\n",
    "            tweet_groups[group_index] = group_dict\n",
    "    return tweet_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47600727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1489a28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:47.068399Z",
     "start_time": "2022-04-27T13:11:47.054399Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_tweet_data(ids, start, end, file):\n",
    "    new_groups = get_tweet_groups_start_end(ids, start, end)\n",
    "    \n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        tweets = json.load(f)\n",
    "        \n",
    "    tweets.update(new_groups)\n",
    "    \n",
    "    with open(file, \"w\",encoding='utf-8') as outfile:\n",
    "        json.dump(tweets, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d72390",
   "metadata": {},
   "source": [
    "### Devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d4176f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T13:20:18.559494Z",
     "start_time": "2022-04-28T13:20:18.539059Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_ids = read_file('../../project-data/dev.data.txt')\n",
    "# dev_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce57ec",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# update_tweet_data(dev_ids, 0, 100, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 100, 200, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 200, 300, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 300, 400, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 400, 500, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 500, 600, \"data/dev_data.json\")\n",
    "# update_tweet_data(dev_ids, 600, 632, \"data/dev_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2058d895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T13:14:35.073151Z",
     "start_time": "2022-04-28T13:14:34.088144Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if data is correct \n",
    "with open(\"data/dev_data_all.json\",'r',encoding='utf-8') as f:\n",
    "    dev_tweets = json.load(f)\n",
    "    \n",
    "# DO NOT PRINT IT DIRECTLY!!!\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7277c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2477f5c8",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc8ca916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:40.567995Z",
     "start_time": "2022-04-27T13:11:40.516970Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids = read_file('../../project-data/train.data.txt')\n",
    "# train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2325fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T14:03:55.622366Z",
     "start_time": "2022-04-27T14:01:52.808469Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# update_tweet_data(train_ids, 0, 100, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 100, 200, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 200, 300, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 300, 400, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 400, 500, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 500, 600, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 600, 700, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 700, 800, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 800, 900, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 900, 1000, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1000, 1100, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1100, 1200, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1200, 1300, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1300, 1400, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1400, 1500, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1500, 1600, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1600, 1700, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1700, 1800, \"data/train_data.json\")\n",
    "# update_tweet_data(train_ids, 1800, 1895, \"data/train_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94aa43c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T15:57:11.145303Z",
     "start_time": "2022-04-27T15:57:10.179301Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if data is correct \n",
    "with open(\"data/train_data_all.json\",'r',encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "# DO NOT PRINT IT DIRECTLY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223c23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8eae17",
   "metadata": {},
   "source": [
    "### Covid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aac25de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T14:44:27.295736Z",
     "start_time": "2022-05-02T14:44:27.086709Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_ids = read_file('../../project-data/covid.data.txt')\n",
    "# covid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9214fb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T17:00:00.884949Z",
     "start_time": "2022-04-28T17:00:00.856950Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(covid_ids)//100 + 1):\n",
    "    \n",
    "    if i == len(covid_ids)//100:\n",
    "        update_tweet_data(covid_ids, 100*i, len(covid_ids), \"data/covid_data_all.json\")     \n",
    "        continue\n",
    "    \n",
    "    update_tweet_data(covid_ids, 100*i, 100*i+100, \"data/covid_data_all.json\")\n",
    "\n",
    "    if i < len(covid_ids)//100:\n",
    "        time.sleep(900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd99aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is correct \n",
    "with open(\"data/covid_data_all.json\",'r',encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "# DO NOT PRINT IT DIRECTLY!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
